There are numerous variants of the genetic algorithm. gago is genetic enough so as to make it possible to easy use different so called *models*. Simply put, a models tells the story of how a GA enhances a population of individuals through a sequence of genetic operators. It does without considering whatsoever the underlying operators. In a nutshell, an evolution model attemps to mimic evolution in the real world. It's extremely important to choose a good model because it is usually the highest influence on the performance of a GA.

## Generational model

The generational model is one the, if not the most, popular models. Simply put it generates $n$ offsprings from a population of size $n$ and replaces the population with the offsprings. The offsprings are generated by selecting 2 individuals from the population and applying a crossover method to the selected individuals until the $n$ offsprings have been generated. The newly generated offsprings are then optionally mutated before replacing the original population. Crossover generates two new individuals, thus if the population size isn't an even number then the second individual from the last crossover (individual $n+1$) won't be included in the new population.

### Diagram

![generational](img/models/generational.png)

### Pseudocode

```
offsprings = ()
while size of offspring < size of population:
    parent1, parent2 = select(population, 2)
    offspring1, offspring2 = crossover(parent1, parent2)
    add offspring1 and offspring2 to offsprings

for each individual of offsprings:
    if rand() < mutationRate:
        mutate(individual)

replace population with offsprings
```

### Operators

| Operator        | Presence |
|-----------------|----------|
| Selector        | Required |
| Crossover       | Required |
| Mutator         | Optional |
| Mutation rate   | Optional |


## Steady state model

The steady state model differs from the generational model in that the entire population isn't replaced between each generations. Instead of adding the children of the selected parents into the next generation, the 2 best individuals out of the two parents and two children are added back into the population so that the population size remains constant. However, one may also replace the parents with the children regardless of their fitness. This method has the advantage of not having to evaluate the newly generated offsprings. Whatsmore, crossover often generates individuals who are sub-par but who have a lot of potential; giving individuals generated from crossover a chance can be beneficial on the long run.

### Diagram

### Pseudocode

```
parent1, parent2 = select(population, 2)
offspring1, offspring2 = crossover(parents)

if keepBest:
    evaluate(offspring1)
    evaluate(offspring2)
    best1, best2 = selectBest(parent1, parent2, offspring1, offspring2)
    replace parent1 with best1
    replace parent1 with best2
else:
    replace parent1 with offspring1
    replace parent1 with offspring2

for each parent:
    if rand() < mutationRate:
        mutate(parent)
```

### Operators

| Operator        | Presence |
|-----------------|----------|
| Selector        | Required |
| Crossover       | Required |
| KeepBest        | Required |
| Mutator         | Optional |
| Mutation rate   | Optional |


## Select down to size

The select down to size method uses two selection rounds. The first one is classic and picks parents to generate new individuals with crossover. However, the offsprings are then added to the original population and a second selection round occurs to determine which individuals will survive to the next generation. Formally $m$ offsprings are generated from a population of $n$, the $n+m$ individuals are then "selected down to size" so that there only remains $n$ individuals. Finally the $n$ newly selected individuals may be mutated.

### Diagram

### Pseudocode

```
offsprings = generateOffsprings(m, individuals, selectorA, crossover)
evaluate(offsprings)
offsprings = merge(offsprings, individuals)
individuals = select(len(individuals), offsprings, selectorB)

for each individual of individuals:
    if rand() < mutationRate:
        mutate(individual)
```

### Operators

| Operator        | Presence |
|-----------------|----------|
| NbrOffsprings   | Required |
| SelectorA       | Required |
| Crossover       | Required |
| SelectorB       | Required |
| Mutator         | Optional |
| Mutation rate   | Optional |


## Island ring

In the island ring model, each individual crosses over with its neighbor in a one-directional ring topology. One of the individuals out of offsprings or the original individual is selected to replace the original individual. Formally, an individual at position $i$ will crossover with it's neighbour at position $i+1$ and generates 2 offsprings. The last individual is connected to the first individual.

### Diagram

### Pseudocode

```
foreach i, individual in individuals:
    neighbour = individuals[i % len(individuals)]
    offspring1, offspring2 = crossover(individual, neighbour)
    selected = select(individual, offspring1, offspring2)
    individuals[i] = selected

for each individual of individuals:
    if rand() < mutationRate:
        mutate(individual)
```

### Operators

| Operator        | Presence |
|-----------------|----------|
| Selector        | Required |
| Crossover       | Required |
| Mutator         | Optional |
| Mutation rate   | Optional |


## Lonely mutant

In this algorithm, a single individual is initialized randomly. Mutation is applied and whenever the mutant is superior to the original, the mutant replaces the original. No crossover occurs.

m <- generate one individual randomly
while stopping criterion has not been met:
    m' <- mutate m
    if fitness(m') > fitness(m)
        m <- m'

## Hill climbing

This is a deterministic hill climbing algorithm. An individual is initialized randomly. While the individual is not at a local optimum, the algorithm takes a ``step" (increments or decrements one of its genes by the step size). If the resulting individual has better fitness, it replaces the original and the step size doubles. If the result is worse, the step size halves. Once a gene is at a local optimum (holding all other genes constant) then stepping begins again on the next gene. When the individual reaches a local optimum, a new solution is randomly generated and hill climbing begins again. The best prior solution is remembered. This algorithm is only intended for problems that take an integer or floating point array as input.

i <- generate an individual randomly
best_so_far <- i
while stopping criterion has not been met:
    get i's bit string and convert it to the problem representation (int or float)
    increment or decrement one of the genes by the step size
    if the resulting individual has higher fitness
        replace i with this individual and increase the step size
    else
        decrease the step size
    if the step size reaches zero and increments and decrements of the current gene have been tested
        move on to the next gene
    if i is at a local optimum
        if fitness(i) > fitness(best_so_far)
            best_so_far <- i

## King of the hill

This algorithm is identical to the hill climber except that once two solutions reach a local optimum, they begin to crossover with each other.

i <- generate an individual randomly
P <- ()
while stopping criterion has not been met:
    get i's bit string and convert it to the problem representation (int or float)
    increment or decrement one of the genes by the step size
    if the resulting individual has higher fitness
        replace i with this individual and increase the step size
    else
        decrease the step size
    if the step size reaches zero and increments and decrements of the current gene have been tested
        move on to the next gene
    if i is at a local optimum
        add i to P
        if there are individuals in P added by crossover that have not climbed their local hills
            i <- the next child of crossover that has not hill climbed
        else
            i <- generate an individual randomly
    if size(P) > 1
        parent1, parent2 <- get two random individuals from P
        child1, child2 <- crossover parent1, parent2
        if the fitness of child1 is greater than the fitness of the worse of the two parents
            add child1 to P
        if the fitness of child2 is greater than the fitness of the worse of the two parents
            add child2 to P
        i <- generate an individual randomly
